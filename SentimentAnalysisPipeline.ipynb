{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysisPipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV5yn515KJCE"
      },
      "source": [
        "## SETUP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU5bAjg6KIEG",
        "outputId": "474b9a60-6dd2-40dd-c41c-f19c83fc294d"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-03 04:08:43--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
            "--2021-12-03 04:08:44--  https://setup.johnsnowlabs.com/colab.sh\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2021-12-03 04:08:44--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1275 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-03 04:08:44 (47.7 MB/s) - written to stdout [1275/1275]\n",
            "\n",
            "setup Colab for PySpark 3.0.3 and Spark NLP 3.3.4\n",
            "Installing PySpark 3.0.3 and Spark NLP 3.3.4\n",
            "\u001b[K     |████████████████████████████████| 209.1 MB 62 kB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 68.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 76.2 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H_hBxXLKR_T",
        "outputId": "01b9bee9-0606-4115-ba4c-0a896fb44fc0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1W6IAzGKVvZ"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIll-XDMKXQf",
        "outputId": "898cd101-9b46-4cc9-f2e1-94672ce4836d"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhd3Ch8VQUO2"
      },
      "source": [
        "spark = sparknlp.start()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5MxxRqusi7K"
      },
      "source": [
        "import re\n",
        "def preprocessor(text):\n",
        "  text = re.sub('<[^>]*>', '', text)\n",
        "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "  text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
        "  return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h7ud0BWIgmP"
      },
      "source": [
        "## PER GENRE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elwdP1YgIPKu"
      },
      "source": [
        "Load IMDB data per genre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXjJorR5ROev"
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "path = r'/content/drive/MyDrive/review_file' # use your path\n",
        "all_files = glob.glob(path + \"/*.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZBbGzRMMktq"
      },
      "source": [
        "df_inputs = {}\n",
        "for i in range(len(all_files)):\n",
        "  df_inputs[i] = pd.read_csv(all_files[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrbQ84aSIPKv"
      },
      "source": [
        "Clean IMDB data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdbIP2gUsbCA"
      },
      "source": [
        "data_cleaned = {}\n",
        "for i in range(len(all_files)):\n",
        "  data = df_inputs[i]\n",
        "  data['remove_mentions'] = data['text'].str.replace('@\\S+','')\n",
        "  stop = stopwords.words('english')\n",
        "  print(data[\"remove_mentions\"])\n",
        "  data['remove_stop'] = data['remove_mentions'].apply(lambda x: \" \".join(x for x in x.split() if x.lower() not in stop))\n",
        "  data['final_reviews'] = data[\"remove_stop\"].apply(lambda x: preprocessor(x))\n",
        "  cleaned_df = pd.DataFrame()\n",
        "  cleaned_df[\"text\"] = data[\"final_reviews\"]\n",
        "  data_cleaned[i] = cleaned_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8vnDZosIPKv"
      },
      "source": [
        "SentimentAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoppN5LvQXs_",
        "outputId": "6ecdd575-bf93-4dbb-d1c7-d994d5f1cc3b"
      },
      "source": [
        "MODEL_NAME='sentimentdl_use_imdb'\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
        " .setInputCols([\"document\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "\n",
        "sentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"sentiment\")\n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "      stages = [\n",
        "          documentAssembler,\n",
        "          use,\n",
        "          sentimentdl\n",
        "      ])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n",
            "sentimentdl_use_imdb download started this may take some time.\n",
            "Approximate size to download 12 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzVOFs56MQnu"
      },
      "source": [
        "result = {}\n",
        "for i in range(len(all_files)):\n",
        "  empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "  pipelineModel = nlpPipeline.fit(empty_df)\n",
        "  df = spark.createDataFrame(data_cleaned[i])\n",
        "  result[i] = pipelineModel.transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7qu-NyKMn5F"
      },
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "for i in range(len(all_files)):\n",
        "  save_res = result[i].select(F.explode(F.arrays_zip('document.result', 'sentiment.result')).alias(\"cols\")).select(F.expr(\"cols['1']\").alias(\"sentiment\")).groupBy(\"sentiment\").count()\n",
        "  res_final = save_res.toDF(\"sentiment\", \"count\")\n",
        "  pandas_res_final = res_final.toPandas()\n",
        "  k = all_files[i][all_files[i].rfind('/')+1:-11]\n",
        "  pandas_res_final.to_csv('/content/drive/MyDrive/IMDB_genre_sentiments_final/Sentiments'+k+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyPJ0Og0HlOC"
      },
      "source": [
        "Load Twitter data per genre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7u4_woNRZMl"
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "path = r'/content/drive/MyDrive/Movies-tweets/Sentiments' # use your path\n",
        "all_files = glob.glob(path + \"/*.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFhId4smR8Al"
      },
      "source": [
        "df_inputs = {}\n",
        "for i in range(len(all_files)):\n",
        "  df_inputs[i] = pd.read_csv(all_files[i],lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynhi1vxGHq04"
      },
      "source": [
        "Clean Twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiymoQ4S50Aj"
      },
      "source": [
        "data_cleaned = {}\n",
        "for i in range(len(all_files)):\n",
        "  data = df_inputs[i]\n",
        "  data['remove_mentions'] = data['tweets'].str.replace('@\\S+','')\n",
        "  stop = stopwords.words('english')\n",
        "  data['remove_stop'] = data['remove_mentions'].apply(lambda x: \" \".join(x for x in x.split() if x.lower() not in stop))\n",
        "  data['final_reviews'] = data[\"remove_stop\"].apply(lambda x: preprocessor(x))\n",
        "  cleaned_df = pd.DataFrame()\n",
        "  cleaned_df[\"text\"] = data[\"final_reviews\"]\n",
        "  data_cleaned[i] = cleaned_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUqQbxpaHutp"
      },
      "source": [
        "SentimentAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oiXzPQU6y_M",
        "outputId": "0663cbd3-5441-4836-8658-a85af0757f99"
      },
      "source": [
        "MODEL_NAME='sentimentdl_use_twitter'\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
        " .setInputCols([\"document\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "\n",
        "sentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"sentiment\")\n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "      stages = [\n",
        "          documentAssembler,\n",
        "          use,\n",
        "          sentimentdl\n",
        "      ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n",
            "sentimentdl_use_twitter download started this may take some time.\n",
            "Approximate size to download 11.4 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrvaII-i6rBU"
      },
      "source": [
        "result = {}\n",
        "for i in range(len(all_files)):\n",
        "  empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "  pipelineModel = nlpPipeline.fit(empty_df)\n",
        "  df = spark.createDataFrame(data_cleaned[i])\n",
        "  result[i] = pipelineModel.transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxVxIhu76tVw"
      },
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "for i in range(len(all_files)):\n",
        "  save_res = result[i].select(F.explode(F.arrays_zip('document.result', 'sentiment.result')).alias(\"cols\")).select(F.expr(\"cols['1']\").alias(\"sentiment\")).groupBy(\"sentiment\").count()\n",
        "  res_final = save_res.toDF(\"sentiment\", \"count\")\n",
        "  pandas_res_final = res_final.toPandas()\n",
        "  k = all_files[i][all_files[i].rfind('/')+1:-4]\n",
        "  pandas_res_final.to_csv('/content/drive/MyDrive/Twitter_genre_sentiment_final/Sentiments'+k+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMDs2LryIco7"
      },
      "source": [
        "## PER MOVIE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4t3-nL3IqJR"
      },
      "source": [
        "Load IMDB data per movie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7x-VlS_fIb7"
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "path = r'/content/drive/MyDrive/imdb_dataset/2_reviews_per_movie_raw' # use your path\n",
        "all_files = glob.glob(path + \"/*.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFuom7hWiReA"
      },
      "source": [
        "df_inputs = {}\n",
        "for i in range(len(all_files)):\n",
        "  df_inputs[i] = pd.read_csv(all_files[i])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGPgkF69IqJR"
      },
      "source": [
        "Clean IMDB data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKwr8D6wiqdl"
      },
      "source": [
        "data_cleaned = {}\n",
        "for i in range(len(all_files)):\n",
        "  data = pd.DataFrame()\n",
        "  data['review'] = df_inputs[i]['review']\n",
        "  # data = df_inputs[i]\n",
        "  data['remove_mentions'] = data['review'].str.replace('@\\S+','')\n",
        "  stop = stopwords.words('english')\n",
        "  # print(data[\"remove_mentions\"])\n",
        "  data['remove_stop'] = data['remove_mentions'].apply(lambda x: \" \".join(x for x in x.split() if x.lower() not in stop))\n",
        "  data['final_reviews'] = data[\"remove_stop\"].apply(lambda x: preprocessor(x))\n",
        "  cleaned_df = pd.DataFrame()\n",
        "  cleaned_df[\"text\"] = data[\"final_reviews\"]\n",
        "  data_cleaned[i] = cleaned_df"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_wvS9nZIqJR"
      },
      "source": [
        "SentimentAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toclXc6AQabY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa66386e-b714-431d-e81a-dffe7e461dd7"
      },
      "source": [
        "MODEL_NAME='sentimentdl_use_imdb'\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
        " .setInputCols([\"document\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "\n",
        "sentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"sentiment\")\n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "      stages = [\n",
        "          documentAssembler,\n",
        "          use,\n",
        "          sentimentdl\n",
        "      ])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n",
            "sentimentdl_use_imdb download started this may take some time.\n",
            "Approximate size to download 12 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIKOMtEsoXI-"
      },
      "source": [
        "result = {}\n",
        "for i in range(len(all_files)):\n",
        "  empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "  pipelineModel = nlpPipeline.fit(empty_df)\n",
        "  df = spark.createDataFrame(data_cleaned[i])\n",
        "  result[i] = pipelineModel.transform(df)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GErAaBGQoaTO"
      },
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "for i in range(len(all_files)):\n",
        "  save_res = result[i].select(F.explode(F.arrays_zip('document.result', 'sentiment.result')).alias(\"cols\")).select(F.expr(\"cols['1']\").alias(\"sentiment\")).groupBy(\"sentiment\").count()\n",
        "  res_final = save_res.toDF(\"sentiment\", \"count\")\n",
        "  pandas_res_final = res_final.toPandas()\n",
        "  k = all_files[i][all_files[i].rfind('/')+1:-4]\n",
        "  pandas_res_final.to_csv('/content/drive/MyDrive/imdb_dataset/3_sentiments_per_movie/'+k+'.csv')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW_2PWvcIqJS"
      },
      "source": [
        "Load Twitter data per genre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anQypkz4IqJS"
      },
      "source": [
        "Clean Twitter data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxHulJHrIqJT"
      },
      "source": [
        "SentimentAnalysis"
      ]
    }
  ]
}